# """
# Kafka S3 Consumer - Äá»c dá»¯ liá»‡u tá»« Kafka vÃ  Ä‘áº©y lÃªn S3 (BRONZE layer)
# """

# import json
# import os
# import gzip
# from datetime import datetime, timezone
# from io import BytesIO
# import time
# from kafka import KafkaConsumer
# from dotenv import load_dotenv
# # import boto3
# # from botocore.exceptions import ClientError

# # Load environment variables
# load_dotenv()


# class S3Consumer:
#     def __init__(self):
#         """
#         Khá»Ÿi táº¡o Kafka Consumer vÃ  S3 client
#         """
#         # Kafka config
#         self.bootstrap_servers = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
#         self.topic = os.getenv('KAFKA_TOPIC', 'air-quality-topic')
#         group_id = 's3-consumer-group'  # KhÃ¡c vá»›i alert consumer group

#         self.consumer = KafkaConsumer(
#             self.topic,
#             bootstrap_servers=self.bootstrap_servers,
#             group_id=group_id,
#             value_deserializer=lambda m: json.loads(m.decode('utf-8')),
#             key_deserializer=lambda k: k.decode('utf-8') if k else None,
#             auto_offset_reset='earliest',
#             enable_auto_commit=True
#         )
#         print(f"âœ“ S3 Consumer Ä‘Ã£ káº¿t ná»‘i tá»›i Kafka: {self.bootstrap_servers}")
#         print(f"âœ“ Topic: {self.topic}")
#         print(f"âœ“ Group ID: {group_id}")

#         # S3 config
#         self.s3_bucket = os.getenv('S3_BUCKET')
#         self.s3_region = os.getenv('S3_REGION', 'us-east-1')
#         self.s3_access_key = os.getenv('AWS_ACCESS_KEY_ID')
#         self.s3_secret_key = os.getenv('AWS_SECRET_ACCESS_KEY')
#         self.s3_endpoint_url = os.getenv('S3_ENDPOINT_URL')  # Cho S3-compatible storage (MinIO, etc.)
        
#         # Batch config
#         self.batch_size = int(os.getenv('S3_BATCH_SIZE', '100'))  # Sá»‘ messages má»—i batch
#         self.batch_timeout = int(os.getenv('S3_BATCH_TIMEOUT', '300'))  # Timeout (giÃ¢y) Ä‘á»ƒ flush batch
        
#         # S3 path prefix
#         self.s3_prefix = os.getenv('S3_PREFIX', 'bronze/openaq/raw')

#         if not self.s3_bucket:
#             raise ValueError("S3_BUCKET environment variable is required")

#         # Initialize S3 client
#         s3_kwargs = {
#             'region_name': self.s3_region,
#         }
        
#         if self.s3_access_key and self.s3_secret_key:
#             s3_kwargs['aws_access_key_id'] = self.s3_access_key
#             s3_kwargs['aws_secret_access_key'] = self.s3_secret_key
        
#         if self.s3_endpoint_url:
#             s3_kwargs['endpoint_url'] = self.s3_endpoint_url

#         self.s3_client = boto3.client('s3', **s3_kwargs)
        
#         # Test S3 connection
#         try:
#             self.s3_client.head_bucket(Bucket=self.s3_bucket)
#             print(f"âœ“ Káº¿t ná»‘i S3 thÃ nh cÃ´ng: {self.s3_bucket}")
#         except ClientError as e:
#             print(f"âš  Cáº£nh bÃ¡o: KhÃ´ng thá»ƒ káº¿t ná»‘i S3: {e}")
#             print("  Tiáº¿p tá»¥c cháº¡y nhÆ°ng cÃ³ thá»ƒ lá»—i khi ghi file...")

#         # Batch storage
#         self.batch = []
#         self.last_flush_time = time.time()

#     def _get_s3_key(self, timestamp_str=None):
#         """
#         Táº¡o S3 key dá»±a trÃªn timestamp (partition theo ngÃ y/giá»)
        
#         Format: bronze/openaq/raw/year=YYYY/month=MM/day=DD/hour=HH/data_YYYYMMDD_HHMMSS.json.gz
#         """
#         if timestamp_str:
#             try:
#                 dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
#             except:
#                 dt = datetime.now(timezone.utc)
#         else:
#             dt = datetime.now(timezone.utc)
        
#         year = dt.strftime('%Y')
#         month = dt.strftime('%m')
#         day = dt.strftime('%d')
#         hour = dt.strftime('%H')
#         timestamp = dt.strftime('%Y%m%d_%H%M%S')
        
#         key = f"{self.s3_prefix}/year={year}/month={month}/day={day}/hour={hour}/data_{timestamp}.json.gz"
#         return key

#     def _compress_data(self, data):
#         """
#         NÃ©n dá»¯ liá»‡u JSON thÃ nh gzip
#         """
#         json_str = json.dumps(data, ensure_ascii=False)
#         buffer = BytesIO()
#         with gzip.GzipFile(fileobj=buffer, mode='wb') as gz:
#             gz.write(json_str.encode('utf-8'))
#         return buffer.getvalue()

#     def _upload_batch_to_s3(self, batch_data, s3_key):
#         """
#         Upload batch data lÃªn S3
#         """
#         try:
#             # Compress data
#             compressed_data = self._compress_data(batch_data)
            
#             # Upload to S3
#             self.s3_client.put_object(
#                 Bucket=self.s3_bucket,
#                 Key=s3_key,
#                 Body=compressed_data,
#                 ContentType='application/json',
#                 ContentEncoding='gzip',
#                 Metadata={
#                     'record_count': str(len(batch_data)),
#                     'uploaded_at': datetime.now(timezone.utc).isoformat()
#                 }
#             )
            
#             print(f"âœ“ ÄÃ£ upload {len(batch_data)} records lÃªn S3: s3://{self.s3_bucket}/{s3_key}")
#             return True
            
#         except ClientError as e:
#             print(f"âœ— Lá»—i khi upload lÃªn S3: {e}")
#             return False
#         except Exception as e:
#             print(f"âœ— Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi upload S3: {e}")
#             return False

#     def _should_flush_batch(self):
#         """
#         Kiá»ƒm tra xem cÃ³ nÃªn flush batch khÃ´ng
#         """
#         # Flush náº¿u Ä‘á»§ sá»‘ lÆ°á»£ng
#         if len(self.batch) >= self.batch_size:
#             return True
        
#         # Flush náº¿u quÃ¡ timeout
#         current_time = time.time()
#         if len(self.batch) > 0 and (current_time - self.last_flush_time) >= self.batch_timeout:
#             return True
        
#         return False

#     def _flush_batch(self):
#         """
#         Flush batch hiá»‡n táº¡i lÃªn S3
#         """
#         if not self.batch:
#             return
        
#         # Láº¥y timestamp tá»« message Ä‘áº§u tiÃªn hoáº·c dÃ¹ng thá»i gian hiá»‡n táº¡i
#         first_timestamp = self.batch[0].get('timestamp') if self.batch else None
#         s3_key = self._get_s3_key(first_timestamp)
        
#         # Upload batch
#         success = self._upload_batch_to_s3(self.batch, s3_key)
        
#         if success:
#             self.batch = []
#             self.last_flush_time = time.time()
#         else:
#             # Náº¿u lá»—i, giá»¯ láº¡i batch Ä‘á»ƒ retry (cÃ³ thá»ƒ implement retry logic sau)
#             print(f"âš  Giá»¯ láº¡i {len(self.batch)} messages Ä‘á»ƒ retry sau")

#     def start_consuming(self):
#         """
#         Báº¯t Ä‘áº§u nháº­n dá»¯ liá»‡u tá»« Kafka vÃ  Ä‘áº©y lÃªn S3
#         """
#         print(f"\n{'='*80}")
#         print(f"Báº®T Äáº¦U S3 CONSUMER - Äáº¨Y Dá»® LIá»†U LÃŠN S3 BRONZE")
#         print(f"{'='*80}")
#         print(f"S3 Bucket: {self.s3_bucket}")
#         print(f"S3 Prefix: {self.s3_prefix}")
#         print(f"Batch Size: {self.batch_size}")
#         print(f"Batch Timeout: {self.batch_timeout}s")
#         print(f"{'='*80}\n")

#         try:
#             for message in self.consumer:
#                 data = message.value
                
#                 # ThÃªm metadata Kafka vÃ o data
#                 data['_kafka_metadata'] = {
#                     'partition': message.partition,
#                     'offset': message.offset,
#                     'timestamp': message.timestamp,
#                     'key': message.key
#                 }
                
#                 # ThÃªm vÃ o batch
#                 self.batch.append(data)
                
#                 print(f"ğŸ“¦ ÄÃ£ nháº­n message: {data.get('location_name', 'Unknown')} "
#                       f"(Batch: {len(self.batch)}/{self.batch_size})")
                
#                 # Kiá»ƒm tra xem cÃ³ nÃªn flush batch khÃ´ng
#                 if self._should_flush_batch():
#                     self._flush_batch()
#                     print(f"  â†’ ÄÃ£ flush batch lÃªn S3\n")

#         except KeyboardInterrupt:
#             print("\nâœ“ Dá»«ng S3 consumer.")
#             # Flush batch cÃ²n láº¡i trÆ°á»›c khi dá»«ng
#             if self.batch:
#                 print(f"  â†’ Äang flush batch cuá»‘i ({len(self.batch)} messages)...")
#                 self._flush_batch()

#         finally:
#             self.close()

#     def close(self):
#         """
#         ÄÃ³ng káº¿t ná»‘i Consumer
#         """
#         print("\nÄang Ä‘Ã³ng S3 Consumer...")
#         # Flush batch cuá»‘i cÃ¹ng
#         if self.batch:
#             print(f"  â†’ Äang flush batch cuá»‘i ({len(self.batch)} messages)...")
#             self._flush_batch()
#         self.consumer.close()
#         print("âœ“ S3 Consumer Ä‘Ã£ Ä‘Ã³ng.")


# def main():
#     """Main function"""
#     try:
#         consumer = S3Consumer()
#         consumer.start_consuming()
#     except ValueError as e:
#         print(f"âœ— Lá»—i cáº¥u hÃ¬nh: {e}")
#         print("\nVui lÃ²ng cáº¥u hÃ¬nh cÃ¡c biáº¿n mÃ´i trÆ°á»ng sau trong file .env:")
#         print("  - S3_BUCKET (báº¯t buá»™c)")
#         print("  - AWS_ACCESS_KEY_ID")
#         print("  - AWS_SECRET_ACCESS_KEY")
#         print("  - S3_REGION (máº·c Ä‘á»‹nh: us-east-1)")
#         print("  - S3_ENDPOINT_URL (tÃ¹y chá»n, cho S3-compatible storage)")
#         print("  - S3_PREFIX (máº·c Ä‘á»‹nh: bronze/openaq/raw)")
#         print("  - S3_BATCH_SIZE (máº·c Ä‘á»‹nh: 100)")
#         print("  - S3_BATCH_TIMEOUT (máº·c Ä‘á»‹nh: 300 giÃ¢y)")


# if __name__ == '__main__':
#     main()

